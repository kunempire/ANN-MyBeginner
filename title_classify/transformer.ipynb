{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "本notebook的transformer模型是BERT网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1、导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/setuptools/sandbox.py:13: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "# 数据科学包\n",
    "import random                      # 随机切分数据集\n",
    "import numpy as np                 # 常用数据科学包\n",
    "import pandas as pd\n",
    "\n",
    "# 深度学习包\n",
    "import paddle\n",
    "import paddle.nn as nn                        # 网络\n",
    "import paddle.nn.functional as F\n",
    "# 导入paddlenlp所需的相关包\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "from paddle.dataset.common import md5file\n",
    "from paddlenlp.datasets import DatasetBuilder # 定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局设置解决matplotlib中文显示错误的问题，参考：https://aistudio.baidu.com/aistudio/projectdetail/1658980\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.font_manager as font_manager\n",
    "# 设置显示中文\n",
    "matplotlib.rcParams['font.sans-serif'] = ['FZSongYi-Z13S'] # 指定默认字体\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False # 解决保存图像是负号'-'显示为方块的问题\n",
    "# 设置字体大小\n",
    "matplotlib.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2、准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 使用pandas读取数据集\n",
    "train = pd.read_table('./data/train.txt', sep='\\t',header=None)  # 训练集\n",
    "dev = pd.read_table('./data/dev.txt', sep='\\t',header=None)      # 验证集\n",
    "test = pd.read_table('./data/test.txt', sep='\\t',header=None)    # 测试集\n",
    "\n",
    "# 由于数据集存放时无列名，因此手动添加列名便于对数据进行更好处理\n",
    "train.columns = [\"text\",'label']\n",
    "dev.columns = [\"text\",'label']\n",
    "test.columns = [\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>网易第三季度业绩低于分析师预期</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>巴萨1年前地狱重现这次却是天堂 再赴魔鬼客场必翻盘</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>美国称支持向朝鲜提供紧急人道主义援助</td>\n",
       "      <td>时政</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>增资交银康联 交行夺参股险商首单</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>午盘：原材料板块领涨大盘</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752466</th>\n",
       "      <td>天津女排奇迹之源竟在场边 他是五冠王真正核心</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752467</th>\n",
       "      <td>北电网络专利拍卖推迟：可能分拆6部分拍卖</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752468</th>\n",
       "      <td>Spirit AeroSystems债券发行价确定</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752469</th>\n",
       "      <td>陆慧明必发火线：法兰克福无胜 曼联国米顺利过关</td>\n",
       "      <td>彩票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752470</th>\n",
       "      <td>首破万元 索尼46寸全新LED液晶特价促</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752471 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text label\n",
       "0                 网易第三季度业绩低于分析师预期    科技\n",
       "1       巴萨1年前地狱重现这次却是天堂 再赴魔鬼客场必翻盘    体育\n",
       "2              美国称支持向朝鲜提供紧急人道主义援助    时政\n",
       "3                增资交银康联 交行夺参股险商首单    股票\n",
       "4                    午盘：原材料板块领涨大盘    股票\n",
       "...                           ...   ...\n",
       "752466     天津女排奇迹之源竟在场边 他是五冠王真正核心    体育\n",
       "752467       北电网络专利拍卖推迟：可能分拆6部分拍卖    科技\n",
       "752468  Spirit AeroSystems债券发行价确定    股票\n",
       "752469    陆慧明必发火线：法兰克福无胜 曼联国米顺利过关    彩票\n",
       "752470       首破万元 索尼46寸全新LED液晶特价促    科技\n",
       "\n",
       "[752471 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看训练集数据，共752471条\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>网民市民集体幻想中奖后如果你中了9000万怎么办</td>\n",
       "      <td>彩票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PVC期货有望5月挂牌</td>\n",
       "      <td>财经</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>午时三刻新作《幻神录―宿命情缘》</td>\n",
       "      <td>游戏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>欧司朗LLFY网络提供一站式照明解决方案</td>\n",
       "      <td>家居</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>试探北京楼市向何方：排不完的队　涨不够的价</td>\n",
       "      <td>房产</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>王大雷看国足比赛预测比分我觉得是2-0或者3-1</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>克雷扎回归猛龙势如破竹希尔遭驱逐太阳惨败51分</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>王建宙将与台商共创4G网络商机</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>普京突访食品超市做调查不满高价猪肉(图)</td>\n",
       "      <td>时政</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>高空俯视女明星性感乳沟(组图)(7)</td>\n",
       "      <td>时尚</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           text label\n",
       "0      网民市民集体幻想中奖后如果你中了9000万怎么办    彩票\n",
       "1                   PVC期货有望5月挂牌    财经\n",
       "2              午时三刻新作《幻神录―宿命情缘》    游戏\n",
       "3          欧司朗LLFY网络提供一站式照明解决方案    家居\n",
       "4         试探北京楼市向何方：排不完的队　涨不够的价    房产\n",
       "...                         ...   ...\n",
       "79995  王大雷看国足比赛预测比分我觉得是2-0或者3-1    体育\n",
       "79996   克雷扎回归猛龙势如破竹希尔遭驱逐太阳惨败51分    体育\n",
       "79997           王建宙将与台商共创4G网络商机    科技\n",
       "79998      普京突访食品超市做调查不满高价猪肉(图)    时政\n",
       "79999        高空俯视女明星性感乳沟(组图)(7)    时尚\n",
       "\n",
       "[80000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看验证集数据，共80000条\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>北京君太百货璀璨秋色 满100省353020元</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>教育部：小学高年级将开始学习性知识</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>专业级单反相机 佳能7D单机售价9280元</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>星展银行起诉内地客户 银行强硬客户无奈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>脱离中国的实际 强压人民币大幅升值只能是梦想</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83594</th>\n",
       "      <td>Razer杯DotA精英挑战赛8月震撼登场</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83595</th>\n",
       "      <td>经济数据好转吹散人民币贬值预期</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83596</th>\n",
       "      <td>抵押率抵押物双控政策 刘明康支招房产贷款</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83597</th>\n",
       "      <td>8000万像素 利图发布Aptus-II 12数码后背</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83598</th>\n",
       "      <td>教育部公布33个国家万余所正规学校名单</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83599 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text\n",
       "0          北京君太百货璀璨秋色 满100省353020元\n",
       "1                教育部：小学高年级将开始学习性知识\n",
       "2            专业级单反相机 佳能7D单机售价9280元\n",
       "3              星展银行起诉内地客户 银行强硬客户无奈\n",
       "4           脱离中国的实际 强压人民币大幅升值只能是梦想\n",
       "...                            ...\n",
       "83594        Razer杯DotA精英挑战赛8月震撼登场\n",
       "83595              经济数据好转吹散人民币贬值预期\n",
       "83596         抵押率抵押物双控政策 刘明康支招房产贷款\n",
       "83597  8000万像素 利图发布Aptus-II 12数码后背\n",
       "83598          教育部公布33个国家万余所正规学校名单\n",
       "\n",
       "[83599 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看测试集数据，共83599条\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 拼接训练和验证集，便于进行统计分析\n",
    "total = pd.concat([train,dev],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "科技    162245\n",
       "股票    153949\n",
       "体育    130982\n",
       "娱乐     92228\n",
       "时政     62867\n",
       "社会     50541\n",
       "教育     41680\n",
       "财经     36963\n",
       "家居     32363\n",
       "游戏     24283\n",
       "房产     19922\n",
       "时尚     13335\n",
       "彩票      7598\n",
       "星座      3515\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 总类别标签分布统计\n",
    "total['label'].value_counts()\n",
    "# 可视化类别标签分布情况\n",
    "# total['label'].value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    832471.000000\n",
       "mean         19.388112\n",
       "std           4.097139\n",
       "min           2.000000\n",
       "25%          17.000000\n",
       "50%          20.000000\n",
       "75%          23.000000\n",
       "max          48.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本长度统计分析,通过分析可以看出文本较短，最长为48\n",
    "total['text'].map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    83599.000000\n",
       "mean        19.815022\n",
       "std          3.883845\n",
       "min          3.000000\n",
       "25%         17.000000\n",
       "50%         20.000000\n",
       "75%         23.000000\n",
       "max         84.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对测试集的长度统计分析，可以看出在长度上分布与训练数据相近\n",
    "test['text'].map(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['科技', '体育', '时政', '股票', '娱乐', '教育', '家居', '财经', '房产', '社会', '游戏', '彩票', '星座', '时尚']\n"
     ]
    }
   ],
   "source": [
    "# 定义要进行分类的14个类别\n",
    "label_list=list(train.label.unique())\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集对应文件及其文件存储格式\n",
    "class NewsData(DatasetBuilder):\n",
    "    SPLITS = {\n",
    "        'train': './data/train.txt',  # 训练集\n",
    "        'dev': './data/dev.txt',      # 验证集\n",
    "    }\n",
    "\n",
    "    def _get_data(self, mode, **kwargs):\n",
    "        filename = self.SPLITS[mode]\n",
    "        return filename\n",
    "\n",
    "    def _read(self, filename):\n",
    "        \"\"\"读取数据\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            head = None\n",
    "            for line in f:\n",
    "                data = line.strip().split(\"\\t\")    # 以'\\t'分隔各列\n",
    "                if not head:\n",
    "                    head = data\n",
    "                else:\n",
    "                    text_a, label = data\n",
    "                    yield {\"text\": text_a, \"label\": label}  # 此次设置数据的格式为：text_a,label，可以根据具体情况进行修改\n",
    "\n",
    "    def get_labels(self):\n",
    "        return label_list   # 类别标签\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.NewsData'>\n"
     ]
    }
   ],
   "source": [
    "# 定义数据集加载函数\n",
    "def load_dataset(name=None,\n",
    "                 data_files=None,\n",
    "                 splits=None,\n",
    "                 lazy=None,\n",
    "                 **kwargs):\n",
    "   \n",
    "    reader_cls = NewsData  # 加载定义的数据集格式\n",
    "    print(reader_cls)\n",
    "    if not name:\n",
    "        reader_instance = reader_cls(lazy=lazy, **kwargs)\n",
    "    else:\n",
    "        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)\n",
    "\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\n",
    "    return datasets\n",
    "\n",
    "# 加载训练和验证集\n",
    "train_ds, dev_ds = load_dataset(splits=[\"train\", \"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据处理函数\n",
    "def convert_example(example, tokenizer, max_seq_length=128, is_test=False):\n",
    "    qtconcat = example[\"text\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)  # tokenizer处理为模型可接受的格式 \n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义dataloader\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    # 训练数据集随机打乱，测试数据集不打乱\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3、准备网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "搭建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1227 13:44:56.629685 59680 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7\n",
      "W1227 13:44:56.646221 59680 gpu_resources.cc:91] device: 0, cuDNN Version: 8.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0, sparse=False)\n",
      "      (position_embeddings): Embedding(512, 768, sparse=False)\n",
      "      (token_type_embeddings): Embedding(16, 768, sparse=False)\n",
      "      (layer_norm): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "      (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "    )\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): LayerList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "        (5): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "        (6): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "        (7): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "        (8): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "        (9): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "        (10): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "        (11): TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, dtype=float32)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
      "  (classifier): Linear(in_features=768, out_features=14, dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RobertaEmbeddings(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size=768,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=16,\n",
    "                 pad_token_id=0):\n",
    "        super(RobertaEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings,\n",
    "                                                hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        if position_ids is None:\n",
    "            # maybe need use shape op to unify static graph and dynamic graph\n",
    "            seq_length = input_ids.shape[1]\n",
    "            position_ids = paddle.arange(0, seq_length, dtype=\"int64\")\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = paddle.zeros_like(input_ids, dtype=\"int64\")\n",
    "\n",
    "        input_embedings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = input_embedings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class RobertaPooler(nn.Layer):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(RobertaPooler, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class RobertaModel(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size=768,\n",
    "                 num_hidden_layers=12,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=16,\n",
    "                 pad_token_id=0):\n",
    "        super(RobertaModel, self).__init__()\n",
    "        self.embeddings = RobertaEmbeddings(\n",
    "            vocab_size, hidden_size, hidden_dropout_prob,\n",
    "            max_position_embeddings, type_vocab_size, pad_token_id)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            hidden_size,\n",
    "            num_attention_heads,\n",
    "            intermediate_size,\n",
    "            dropout=hidden_dropout_prob,\n",
    "            activation=hidden_act,\n",
    "            attn_dropout=attention_probs_dropout_prob,\n",
    "            act_dropout=0)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_hidden_layers)\n",
    "        self.pooler = RobertaPooler(hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, position_ids, token_type_ids):\n",
    "        embeddings = self.embeddings(input_ids, position_ids, token_type_ids)\n",
    "        encoder_output = self.encoder(embeddings)\n",
    "        pooled_output = self.pooler(encoder_output)\n",
    "        return pooled_output\n",
    "\n",
    "class RobertaForSequenceClassification(nn.Layer):\n",
    "    def __init__(self, vocab_size, num_labels=14, embed_size=768, num_layers=12):\n",
    "        super(RobertaForSequenceClassification, self).__init__()\n",
    "        self.roberta = RobertaModel(vocab_size)\n",
    "        self.dropout = nn.Dropout(p=0.1, mode=\"upscale_in_train\")\n",
    "        self.classifier = nn.Linear(embed_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, position_ids, token_type_ids):\n",
    "        roberta_output = self.roberta(input_ids, position_ids, token_type_ids)\n",
    "        pooled_output = self.dropout(roberta_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# 创建模型实例\n",
    "vocab_size = 21128  # 词汇表大小\n",
    "num_labels = 14  # 分类标签数量\n",
    "model = RobertaForSequenceClassification(vocab_size, num_labels)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "载入预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PaddleNLP不仅支持RoBERTa预训练模型，还支持ERNIE、BERT、Electra等预训练模型。\n",
    "\n",
    "更多支持模型请查看：[PaddleNLP Transformer预训练模式](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-12-27 13:45:00,998] [    INFO]\u001b[0m - Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"cls_token_id\": 101,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[33m[2023-12-27 13:45:09,196] [ WARNING]\u001b[0m - Some weights of the model checkpoint at hfl/roberta-wwm-ext-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[33m[2023-12-27 13:45:09,207] [ WARNING]\u001b[0m - Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at hfl/roberta-wwm-ext-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32m[2023-12-27 13:45:09,215] [    INFO]\u001b[0m - Already cached /root/.paddlenlp/models/hfl/roberta-wwm-ext-large/vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-12-27 13:45:09,260] [    INFO]\u001b[0m - tokenizer config file saved in /root/.paddlenlp/models/hfl/roberta-wwm-ext-large/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-12-27 13:45:09,265] [    INFO]\u001b[0m - Special tokens file saved in /root/.paddlenlp/models/hfl/roberta-wwm-ext-large/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 下载预训练模型\n",
    "MODEL_NAME = \"hfl/roberta-wwm-ext-large\"\n",
    "model = ppnlp.transformers.RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=14) \n",
    "# 定义模型对应的tokenizer，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。需注意tokenizer类要与选择的模型相对应，具体可以查看PaddleNLP相关文档\n",
    "tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_state_dict(paddle.load('./pretrained_models/transformer/final.pdparams'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 48\n",
    "# python中的偏函数partial，把一个函数的某些参数固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "# 训练集迭代器\n",
    "train_dataloader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "# 验证集迭代器\n",
    "valid_dataloader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义超参，loss，优化器等\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "# 定义训练配置参数：\n",
    "# 定义训练过程中的最大学习率\n",
    "learning_rate = 4e-5\n",
    "# 训练轮次\n",
    "epochs = 2\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.0\n",
    "\n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "\n",
    "# AdamW优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "metric = paddle.metric.Accuracy()              # accuracy评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义模型训练验证评估函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    loss = np.mean(losses)\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (loss, accu))  # 输出验证集上评估效果\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return accu, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddle.fluid.libpaddle.Generator at 0x7f6553288170>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 固定随机种子便于结果的复现\n",
    "seed = 1024\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "paddle.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "from visualdl import LogWriter\n",
    "logwriter = LogWriter(logdir='./visualdl/transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 模型训练：\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "save_dir = \"./model/transformer\"\n",
    "if not  os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "pre_accu=0\n",
    "accu=0\n",
    "global_step = 0\n",
    "for epoch in range(0, epochs):\n",
    "    train_batchs_per_epoch = len(train_dataloader)\n",
    "    for step, batch in enumerate(train_dataloader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "        # 记录当前训练 到 VisualDL\n",
    "        logwriter.add_scalar(\"train_loss\", value=loss, step=step+epoch*(train_batchs_per_epoch))\n",
    "        logwriter.add_scalar(\"train_acc\", value=acc, step=step+epoch*(train_batchs_per_epoch))\n",
    "\n",
    "    # 每轮结束对验证集进行评估\n",
    "    avg_acc, avg_loss = evaluate(model, criterion, metric, valid_dataloader)\n",
    "    print(avg_acc)\n",
    "    if avg_acc > pre_accu:\n",
    "        # 保存较上一轮效果更优的模型参数\n",
    "        save_param_path = os.path.join(save_dir, 'final.pdparams')  # 保存模型参数\n",
    "        paddle.save(model.state_dict(), save_param_path)\n",
    "        pre_accu=avg_acc\n",
    "     \n",
    "    #记录当前测试集平均 Loss 和准确率到 VisualDL\n",
    "    logwriter.add_scalar(\"eval_acc\", value=avg_acc, step=epoch)\n",
    "    logwriter.add_scalar(\"eval_loss\", value=avg_loss, step=epoch)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5、模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = './model/transformer/final.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型预测函数\n",
    "def predict(model, data, tokenizer, label_map, batch_size=1):\n",
    "    examples = []\n",
    "    # 将输入数据（list格式）处理为模型可接受的格式\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=128,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results  # 返回预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义要进行分类的类别\n",
    "label_list=list(train.label.unique())\n",
    "label_map = { \n",
    "    idx: label_text for idx, label_text in enumerate(label_list)\n",
    "}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from ./model/transformer/final.pdparams\n",
      "eval loss: 0.09429, accu: 0.96850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.968499606245078, 0.094293155)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试最优模型参数在验证集上的分数\n",
    "evaluate(model, criterion, metric, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "测试集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取要进行预测的测试集文件\n",
    "test = pd.read_table('./data/test.txt',sep='\\t', header=None) \n",
    "test.columns = [\"text\"]\n",
    "# 定义对数据的预处理函数,处理为模型输入指定list格式\n",
    "def preprocess_prediction_data(data):\n",
    "    examples = []\n",
    "    for text in data:\n",
    "        examples.append({\"text\": text})\n",
    "    return examples\n",
    "\n",
    "# 对测试集数据进行格式处理\n",
    "data1 = list(test.text)\n",
    "examples = preprocess_prediction_data(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 对测试集进行预测\n",
    "results = predict(model, examples, tokenizer, label_map, batch_size=16)   \n",
    "\n",
    "# 将list格式的预测结果存储为txt文件，提交格式要求：每行一个类别\n",
    "def write_results(labels, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf8\") as f:\n",
    "        f.writelines(\"\\n\".join(labels))\n",
    "\n",
    "write_results(results, \"./result.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result.txt (deflated 89%)\n"
     ]
    }
   ],
   "source": [
    "# 因格式要求为zip，故需要将结果文件压缩为submission.zip提交文件\n",
    "!zip 'submission.zip' 'result.txt'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
